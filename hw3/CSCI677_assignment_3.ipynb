{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMblnPgI4pQh"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this assignment you will practice putting together an image classification pipeline based on CNNs for [CIFAR-10 and/or CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. The goals of this assignment are as follows:\n",
    "\n",
    "\n",
    "\n",
    "*   Understand the components of a CNN model and a Vision Transformer (ViT) model.\n",
    "*   Understand how to modify a standard CNN model towards a specific task.\n",
    "*   Implement a basic neural network training pipeline in Pytorch.\n",
    "*   Implement and train an AlexNet model.\n",
    "*   Implement and train a ResNet model.\n",
    "*   Implement and train a ViT model.\n",
    "*   Understand the differences and tradeoffs between these models.\n",
    "\n",
    "Please fill in all the **TODO** code blocks. Once you are ready to submit:\n",
    "\n",
    "* Export the notebook `CSCI677_assignment_3.ipynb` as a PDF `[Your USC ID]_CSCI677_assignment_3.pdf`\n",
    "\n",
    "Please make sure that the notebook have been run before exporting PDF, and your code and all cell outputs are visible in your submitted PDF. Regrading request will not be accepted if your code/output is not visible in the original submission. Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3_vNZ33-kcr"
   },
   "source": [
    "In case you haven't installed PyTorch yet, run the following command to install torch and torchvision."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vhaHm9fe-vYH",
    "ExecuteTime": {
     "end_time": "2025-03-18T03:36:48.256321Z",
     "start_time": "2025-03-18T03:36:47.640188Z"
    }
   },
   "source": [
    "!pip install torch torchvision"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (2.6.0)\r\n",
      "Requirement already satisfied: torchvision in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (0.21.0)\r\n",
      "Requirement already satisfied: filelock in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (2025.3.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torchvision) (2.2.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from torchvision) (11.1.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sumeet/miniconda3/envs/cs677/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\r\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:36:48.259392Z",
     "start_time": "2025-03-18T03:36:48.257421Z"
    }
   },
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ucCCRiq6BVp"
   },
   "source": [
    "# **Data Preparation**\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) is a well known dataset composed of 60,000 colored 32x32 images in 10 classes, with 6000 images per class. The utility function `cifar10()` returns the entire CIFAR-10 dataset as a set of four Torch tensors:\n",
    "* `x_train` contains all training images (real numbers in the range  [0,1] )\n",
    "* `y_train` contains all training labels (integers in the range  [0,9] )\n",
    "* `x_test` contains all test images\n",
    "* `y_test` contains all test labels\n",
    "\n",
    "This function automatically downloads the CIFAR-10 dataset the first time you run it.\n",
    "\n",
    "[CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) is just like the CIFAR-10 dataset, except it has 100 classes containing 600 images each. Below we provided wrapper classes for CIFAR-10 and CIFAR-100 datasets. You can choose one or both of them for training your CNNs. If you choose one of them, use the same one to train all your models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qh-a1XGO9qNF",
    "ExecuteTime": {
     "end_time": "2025-03-18T03:36:48.288374Z",
     "start_time": "2025-03-18T03:36:48.260139Z"
    }
   },
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CIFAR10Dataset:\n",
    "    def __init__(self, batch_size=128, root=\"data\"):\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))]\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.training_data = datasets.CIFAR10(\n",
    "            root=root,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=self.transform\n",
    "        )\n",
    "        self.train_dataloader = DataLoader(self.training_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.test_data = datasets.CIFAR10(\n",
    "            root=root,\n",
    "            train=False,\n",
    "            download=False,\n",
    "            transform=self.transform\n",
    "        )\n",
    "        self.test_dataloader = DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        self.classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "class CIFAR100Dataset:\n",
    "    def __init__(self, batch_size=128, root=\"data\"):\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))]  # CIFAR-100 normalization values\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.training_data = datasets.CIFAR100(\n",
    "            root=root,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=self.transform\n",
    "        )\n",
    "        self.train_dataloader = DataLoader(self.training_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.test_data = datasets.CIFAR100(\n",
    "            root=root,\n",
    "            train=False,\n",
    "            download=False,\n",
    "            transform=self.transform\n",
    "        )\n",
    "        self.test_dataloader = DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        self.classes = self.training_data.classes\n"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:36:48.806564Z",
     "start_time": "2025-03-18T03:36:48.804880Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:36:49.065974Z",
     "start_time": "2025-03-18T03:36:49.063212Z"
    }
   },
   "source": [
    "# Function to count the number of trainable parameters in a model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage\n",
    "model = torch.nn.Linear(10, 2)  # Example model\n",
    "print(f\"Number of parameters: {count_parameters(model)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 22\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZalEn-ux99oz"
   },
   "source": [
    "# AlexNet (20 pts)\n",
    "AlexNet, introduced by Alex Krizhevsky in 2012, marked a significant breakthrough in deep learning for computer vision. This deep convolutional neural network consists of five convolutional layers, some followed by max-pooling layers, and three fully connected layers. AlexNet was designed for large-scale image classification tasks and was notably successful in the ImageNet Large Scale Visual Recognition Challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2nZE65W_PBU"
   },
   "source": [
    "## Implement AlexNet (20 pts)\n",
    "Classical AlexNet architecture is as follows:\n",
    "\n",
    "\n",
    "![LeNet-5 Architecture](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*wgJ9iOjl_JzjOZ3e9jDFAw.png)\n",
    "\n",
    "\n",
    "The original AlexNet was designed for high-resolution images (224x224x3) from the ImageNet dataset. However, the CIFAR-10 and CIFAR-100 datasets consist of lower-resolution images (32x32x3). To adapt AlexNet for these datasets, you need to modify it.\n",
    "\n",
    "Requirements:\n",
    "* **Input Adaptation**: Modify the network to accept 32x32x3 input dimensions, suitable for CIFAR-10 and CIFAR-100 images.\n",
    "* **Architecture**: Implement a network with the following layers:\n",
    "\n",
    "  (Convolutional Layer 1 -> ReLU -> Max Pooling 1) ->\n",
    "\n",
    "  (Convolutional Layer 2 -> ReLU -> Max Pooling 2) ->\n",
    "\n",
    "  (Convolutional Layer 3 -> ReLU -> Convolutional Layer 4 -> ReLU -> Convolutional Layer 5 -> ReLU -> Max Pooling 3) ->\n",
    "\n",
    "  Flattening ->\n",
    "\n",
    "  (Linear -> ReLU) ->\n",
    "\n",
    "  (Linear -> ReLU) -> Linear.\n",
    "* Use you can design your own convolution filters and max pooling layers.\n",
    "* Your model must contains less than **40 Million** parameters. We provide `count_parameters()` function to count the number of parameters in a model.\n",
    "\n",
    "**Hint**: you can use nn.Sequential() to simplify your implementation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LdieMH2P9U1W",
    "ExecuteTime": {
     "end_time": "2025-03-18T03:36:50.415918Z",
     "start_time": "2025-03-18T03:36:50.412239Z"
    }
   },
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=3), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=3), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(512, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.mlp(x)\n",
    "        return x \n"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:36:51.771330Z",
     "start_time": "2025-03-18T03:36:51.759572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AlexNet()\n",
    "x = torch.randn((1, 3, 32, 32))\n",
    "print(model.cnn(x).shape)\n",
    "model(x)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0133,  0.0565, -0.0720,  0.0266, -0.0618,  0.0786, -0.0128,  0.0264,\n",
       "          0.0949, -0.0431]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4o3qwInhsnT"
   },
   "source": [
    "# ResNet (20 pts)\n",
    "ResNet, short for Residual Network, was introduced in 2015 by Kaiming He et al. At its core, ResNet introduces the concept of residual blocks, which allows gradients to flow directly through the network's many layers. In comparison to earlier architectures like AlexNet, ResNet's approach demonstrates the transformative power of residual connections.\n",
    "\n",
    "In this section, you will implement ResNet-18 for CIFAR-10/100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LQbwuLUj1mO"
   },
   "source": [
    "## Implement Residual Block (10 pts)\n",
    "The Residual Block is a crucial component in ResNet. It works by introducing a shortcut connection, also known as a skip connection, alongside a regular neural network layer. This shortcut connection enables the flow of information directly from one layer to another, bypassing some intermediate layers.\n",
    "\n",
    "The key idea is to learn a residual function, which represents the difference between the desired output and the current output of the block. By doing so, the block aims to make the output closer to what it should be. This approach mitigates the vanishing gradient problem, which can occur in very deep networks, making it easier to train deep models effectively.\n",
    "\n",
    "![Residual Block](https://miro.medium.com/v2/resize:fit:1140/format:webp/1*6WlIo8W1_Qc01hjWdZy-1Q.png)\n",
    "\n",
    "\n",
    "The weight layer usually consists of a convolutional layer and a batch normalization layer. The batch normalization layer, often abbreviated as BatchNorm, normalizes the input of a neural network layer across a mini-batch of data during training. BatchNorm not only accelerates convergence but also acts as a form of regularization, reducing the risk of overfitting. In PyTorch, it is implemented by nn.BatchNorm2d().\n",
    "\n",
    "You are asked to implement the residual block with the following requirements:\n",
    "* The residual block takes input of size n * n * `in_channels` and output m * m * `out_channels` with m = (n-1) / `stride` + 1\n",
    "* The residual function consists of the following components:\n",
    "\n",
    "  Conv -> BatchNorm -> ReLU -> Conv -> BatchNorm\n",
    "\n",
    "  where Conv means 3x3 convolutional filters with padding 1. If `stride` != 1, set stride for the first Conv.\n",
    "* The shortcut should be identity if `in_channels` == `out_channels` and `stride` == 1. Otherwise, it should be a convolutional layer with kernel_size=1 and stride=`stride`.\n",
    "* After adding the residual function and the shortcut, apply another ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "56act4Fdtbxl",
    "ExecuteTime": {
     "end_time": "2025-03-18T03:37:01.671239Z",
     "start_time": "2025-03-18T03:37:01.667661Z"
    }
   },
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.skip = nn.Identity() if (stride == 1 and in_channels == out_channels) else nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        self.final_act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        res = self.block(x)\n",
    "        res = res + identity\n",
    "        res = self.final_act(res) \n",
    "        return res "
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RYB3dvEzavj"
   },
   "source": [
    "## Implement ResNet (10 pts)\n",
    "ResNet-18 is part of the ResNet family, known for its exceptional depth and performance in image classification tasks. It consists of 18 layers, beginning with one convolutional layer, followed by a few residual blocks, and ending with a fully-connected layer. Here is a glimpse of its architecture:\n",
    "\n",
    "\n",
    "![ResNet-18](https://www.researchgate.net/profile/Sajid-Iqbal-13/publication/336642248/figure/fig1/AS:839151377203201@1577080687133/Original-ResNet-18-Architecture.png)\n",
    "\n",
    "\n",
    "In this part of the assignment, you are asked to implement a modified ResNet for CIFAR-10/100. Requirements:\n",
    "* The model should take inputs of 32x32x3 and output a vector of dimension equal to the number of classes (10 for CIFAR-10 and 100 for CIFAR-100).\n",
    "* The model should begin with a convolutional layer with kernel_size=3 and padding=1:\n",
    "\n",
    "  Conv -> BatchNorm -> ReLU\n",
    "\n",
    "  The output size should be 64x32x32.\n",
    "* After the first layer, append with 4 residual blocks such that the output size changes as follows:\n",
    "  \n",
    "  (Input size after previous step) 64x32x32 -> 64x32x32 -> 256x16x16 -> 256x8x8 -> 512x2x2\n",
    "* The model should end with average pooling (kernel_size=2), flattening, and a fully-connected layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "znZV9y734trg",
    "ExecuteTime": {
     "end_time": "2025-03-18T03:36:26.951676Z",
     "start_time": "2025-03-18T03:36:26.947981Z"
    }
   },
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.proj_in = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.trunk = nn.Sequential(\n",
    "            ResidualBlock(in_channels=64, out_channels=64, stride=1),\n",
    "            ResidualBlock(in_channels=64, out_channels=256, stride=2),\n",
    "            ResidualBlock(in_channels=256, out_channels=256, stride=2),\n",
    "            ResidualBlock(in_channels=256, out_channels=512, stride=4),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj_in(x)\n",
    "        x = self.trunk(x) \n",
    "        return x "
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:25:41.611566Z",
     "start_time": "2025-03-18T03:25:41.609872Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (20 pts)\n",
    "The Vision Transformer (ViT), introduced in 2020 by Dosovitskiy et al., applies the Transformer architectures, originally designed for natural language processing, to visual data.\n",
    "\n",
    "![ViT](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q0tvs1aDxi_7Otm_Zgys1A.png)\n",
    "\n",
    "In this section, you are tasked with implementing a Vision Transformer model for the CIFAR-10 or CIFAR-100 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Patch Embedding Block with Positional Encoding (10 pts)\n",
    "\n",
    "In Vision Transformers (ViTs), the Patch Embedding Block converts input images into a sequence of patch embeddings, enabling the model to process image data using transformer architectures. Since transformers are not inherently aware of the spatial relationships between patches, positional encoding is added to provide this information.\n",
    "\n",
    "Overview:\n",
    "- **Input:** 3x32x32 images, **Arguments:**: `patch_size` (make sure `32 % patch_size == 0` ), `embed_dim`\n",
    "- Divide the image into non-overlapping patches of size `3 x patch_size x patch_size`. You should end up getting `(32 // patch_size)**2` patches.\n",
    "- Flatten the pixels in each patch (into a single dimension of size `3 x patch_size x patch_size`), apply Layer Normalization, project it into a higher-dimensional space (e.g., 256 dimensions) using a fully-connected layer, and then apply another Layer Normalization.\n",
    "- Add positional encodings to the patch embeddings to retain spatial information.\n",
    "\n",
    "You are asked to implement the Patch Embedding Block as follow:\n",
    "- Transform \"b c (h x p) (w x p) -> b (h x w) (p x p x c)\" where b is batch size, c is number of channels, h x p = 32 is the input image height, w x p = 32 is the input image width, and p is the patch size (e.g., 4).\n",
    "- \"b (h x w) (p x p x c)\" -> LayerNorm -> fully-connected layer -> LayerNorm\n",
    "- add positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:44:08.881602Z",
     "start_time": "2025-03-18T03:44:08.878089Z"
    }
   },
   "source": [
    "def get_positional_encoding(seq_len, embed_dim):\n",
    "    # refer to this paper https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "    # follow the original implementation proposed in Section 3.5 \n",
    "    pe = torch.zeros(seq_len, embed_dim)\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, embed_dim, 2):\n",
    "            pe[pos, i] = torch.sin(torch.Tensor([pos]) / 10_000. ** (2. * i / embed_dim))\n",
    "            pe[pos, i + 1] = torch.cos(torch.Tensor([pos]) / 10_000. ** (2. * i / embed_dim))\n",
    "    return pe\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim \n",
    "        self.norm1 = nn.LayerNorm(3 * patch_size**2)\n",
    "        self.proj = nn.Linear(3 * patch_size ** 2, embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        patches = einops.rearrange(x, 'b c nh nw ph pw -> b (nh nw) (c ph pw)')\n",
    "        patches = self.norm1(patches)\n",
    "        embeds = self.proj(patches)\n",
    "        embeds = self.norm2(embeds)\n",
    "        pe = get_positional_encoding(embeds.shape[1], self.embed_dim).to(x.device)\n",
    "        embeds += pe \n",
    "        return embeds\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:24:57.548997Z",
     "start_time": "2025-03-18T03:24:57.479803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import einops\n",
    "img = torch.randn((1, 3, 32, 32))\n",
    "img_patches = img.unfold(2, 8, 8).unfold(3, 8, 8)  # (B x C x NP x NP x PS x PS) \n",
    "img_patches = einops.rearrange(img_patches, 'b c nh nw ph pw -> b (nh nw) (c ph pw)')\n",
    "img_patches.shape\n",
    "pe = PatchEmbedding(patch_size=8, embed_dim=256)\n",
    "print(pe(img).shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 256])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Vision Transformer (ViT) (10 pts)\n",
    "\n",
    "The Vision Transformer (ViT) model comprises three components (refer to figure above):\n",
    "\n",
    "1. **Patch Embedding:** Converts input images into a sequence of patch embeddings.\n",
    "\n",
    "2. **Transformer Encoder:** Processes the sequence of patch embeddings to capture complex patterns and relationships.\n",
    "\n",
    "3. **MLP Head:** Maps the output from the Transformer Encoder to class predictions.\n",
    "\n",
    "**Implementation Requirements:**\n",
    "\n",
    "- **Input and Output Dimensions:** The model should accept inputs of size 32x32x3 and output a vector with a dimension equal to the number of classes (10 for CIFAR-10 and 100 for CIFAR-100).\n",
    "\n",
    "- **Patch Embedding:** Begin with the PatchEmbedding module you previously implemented.\n",
    "\n",
    "- **Transformer Encoder:** Utilize `nn.TransformerEncoder()` to process the sequence of patch embeddings, capturing high-level representations.\n",
    "\n",
    "- **MLP Head:** Conclude with an mean pooling operation over the temporal dimension (dimension 1), followed by a Multi-Layer Perceptron (MLP) head that maps the pooled embeddings to class predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:48:19.326038Z",
     "start_time": "2025-03-18T03:48:19.323288Z"
    }
   },
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=512, depth=6, num_heads=8, num_classes=10):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.pe = PatchEmbedding(patch_size, embed_dim)\n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=depth)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.pe(x)\n",
    "        embeds = self.transformer(embeds)\n",
    "        embeds = embeds.mean(dim=1)\n",
    "        preds = self.mlp(embeds)\n",
    "        return preds "
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urxGfv7h7YPG"
   },
   "source": [
    "# Training Neural Networks (20 pts)\n",
    "In this section, you will implement a `Trainer` class, use it to train the models that you defined previously, and evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9anAco48Aht"
   },
   "source": [
    "## Check CUDA and GPUs\n",
    "The following code helps you check if CUDA is available and lists the available GPUs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NYlsO8GA90vz",
    "ExecuteTime": {
     "end_time": "2025-03-18T03:25:00.115006Z",
     "start_time": "2025-03-18T03:25:00.029663Z"
    }
   },
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "    # Get the name of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "\n",
    "    # Set the current GPU device\n",
    "    device = torch.cuda.current_device()\n",
    "    print(f\"Current GPU device: {device} - {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3090\n",
      "Current GPU device: 0 - NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW7k1eFy_c1U"
   },
   "source": [
    "## Complete the Trainer Class (15 pts)\n",
    "Fill-in all the TODOs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-jD3MKbt9vn1",
    "ExecuteTime": {
     "end_time": "2025-03-18T03:48:33.111744Z",
     "start_time": "2025-03-18T03:48:33.107465Z"
    }
   },
   "source": [
    "class Trainer:\n",
    "    def __init__(self, dataset, net, optimizer, loss_function=nn.CrossEntropyLoss(),\n",
    "                 device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.dataset = dataset\n",
    "        self.net = net.to(device)\n",
    "        self.lossFunction = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        total_loss = 0.\n",
    "        for batch, labels in self.dataset.train_dataloader: \n",
    "            batch = batch.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            preds = self.net(batch)\n",
    "            loss = self.lossFunction(preds, labels)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss \n",
    "        return total_loss\n",
    "\n",
    "    def compute_test_accuracy(self, path):\n",
    "        accs = []\n",
    "        for batch, labels in self.dataset.test_dataloader:\n",
    "            batch = batch.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            preds = self.net(batch)\n",
    "            pred_labels = F.softmax(preds, dim=1).argmax(dim=1)\n",
    "            acc = (pred_labels == labels).float().mean()\n",
    "            accs.append(acc)\n",
    "        avg_acc = sum(accs) / len(accs)\n",
    "        torch.save(self.net, path)\n",
    "        print(f'Average Accuracy: {avg_acc * 100:.2f}%')\n",
    "        return avg_acc\n",
    "\n",
    "    def train(self, path, num_epochs=20):\n",
    "      self.net.train()  # Set model to training mode\n",
    "      best_accuracy = 0.0\n",
    "      for epoch in range(num_epochs):\n",
    "          total_loss = self.train_one_epoch()\n",
    "          print(f'Loss: {total_loss:.2f}')\n",
    "          if epoch > 0 and epoch % 5 == 0: \n",
    "              avg_acc = self.compute_test_accuracy(path)\n",
    "              if avg_acc > best_accuracy:\n",
    "                  best_accuracy = avg_acc"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj6DxGwvALFu"
   },
   "source": [
    "## Training (5 pts)\n",
    "Follow these steps to train and evaluate your models (AlexNet, ResNet, and ViT):\n",
    "* Create the model, the dataset, and the optimizer. We suggest using SGD with a learning rate of `1e-2`, but you are welcome to explore other options.\n",
    "* Configure the trainer.\n",
    "* Compute and print test accuracy before training.\n",
    "* Train the model.\n",
    "* Compute and print test accuracy after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GLSfjg89955p",
    "ExecuteTime": {
     "end_time": "2025-03-18T04:00:56.307656Z",
     "start_time": "2025-03-18T03:50:28.614145Z"
    }
   },
   "source": [
    "dataset = CIFAR10Dataset()\n",
    "cp_path = './checkpoints/alexnet.pt'\n",
    "# AlexNet train and evaluation\n",
    "print('############ AlexNet ###############')\n",
    "# alexnet = AlexNet(num_classes=10)\n",
    "# optim = torch.optim.Adam(alexnet.parameters(), lr=1e-3)\n",
    "# alexnet_trainer = Trainer(dataset, alexnet, optim)\n",
    "# alexnet_trainer.compute_test_accuracy(cp_path)\n",
    "# alexnet_trainer.train(cp_path)\n",
    "# ResNet train and evaluation\n",
    "print('############ ResNet ###############')\n",
    "# cp_path = './checkpoints/resnet.pt'\n",
    "# resnet = ResNet(num_classes=10)\n",
    "# optim = torch.optim.Adam(resnet.parameters(), lr=1e-3)\n",
    "# resnet_trainer = Trainer(dataset, resnet, optim)\n",
    "# resnet_trainer.compute_test_accuracy(cp_path)\n",
    "# resnet_trainer.train(cp_path)\n",
    "\n",
    "# ViT train and evaluation\n",
    "print('############ ViT ###############')\n",
    "cp_path = './checkpoints/vit.pt'\n",
    "vit = VisionTransformer(depth=1)\n",
    "optim = torch.optim.Adam(vit.parameters(), lr=1e-3)\n",
    "vit_trainer = Trainer(dataset, vit, optim)\n",
    "vit_trainer.compute_test_accuracy(cp_path)\n",
    "vit_trainer.train(cp_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ AlexNet ###############\n",
      "############ ResNet ###############\n",
      "############ ViT ###############\n",
      "Average Accuracy: 6.10%\n",
      "Loss: 672.07\n",
      "Loss: 541.00\n",
      "Loss: 482.46\n",
      "Loss: 444.26\n",
      "Loss: 420.28\n",
      "Loss: 396.21\n",
      "Average Accuracy: 60.67%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[59], line 26\u001B[0m\n\u001B[1;32m     24\u001B[0m vit_trainer \u001B[38;5;241m=\u001B[39m Trainer(dataset, vit, optim)\n\u001B[1;32m     25\u001B[0m vit_trainer\u001B[38;5;241m.\u001B[39mcompute_test_accuracy(cp_path)\n\u001B[0;32m---> 26\u001B[0m \u001B[43mvit_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcp_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[57], line 41\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, path, num_epochs)\u001B[0m\n\u001B[1;32m     39\u001B[0m best_accuracy \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m---> 41\u001B[0m     total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m: \n",
      "Cell \u001B[0;32mIn[57], line 15\u001B[0m, in \u001B[0;36mTrainer.train_one_epoch\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     13\u001B[0m batch \u001B[38;5;241m=\u001B[39m batch\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     14\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m---> 15\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlossFunction(preds, labels)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/miniconda3/envs/cs677/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/cs677/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[55], line 14\u001B[0m, in \u001B[0;36mVisionTransformer.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 14\u001B[0m     embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer(embeds)\n\u001B[1;32m     16\u001B[0m     embeds \u001B[38;5;241m=\u001B[39m embeds\u001B[38;5;241m.\u001B[39mmean(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/cs677/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/cs677/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[47], line 26\u001B[0m, in \u001B[0;36mPatchEmbedding.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     24\u001B[0m embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj(patches)\n\u001B[1;32m     25\u001B[0m embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(embeds)\n\u001B[0;32m---> 26\u001B[0m pe \u001B[38;5;241m=\u001B[39m \u001B[43mget_positional_encoding\u001B[49m\u001B[43m(\u001B[49m\u001B[43membeds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(x\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     27\u001B[0m embeds \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m pe \n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m embeds\n",
      "Cell \u001B[0;32mIn[47], line 7\u001B[0m, in \u001B[0;36mget_positional_encoding\u001B[0;34m(seq_len, embed_dim)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pos \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(seq_len):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, embed_dim, \u001B[38;5;241m2\u001B[39m):\n\u001B[0;32m----> 7\u001B[0m         pe[pos, i] \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mpos\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10_000.\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2.\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43membed_dim\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m         pe[pos, i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcos(torch\u001B[38;5;241m.\u001B[39mTensor([pos]) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m10_000.\u001B[39m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m2.\u001B[39m \u001B[38;5;241m*\u001B[39m i \u001B[38;5;241m/\u001B[39m embed_dim))\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pe\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FX2-yUziAi5w"
   },
   "source": [
    "## Evaluation using Confusion Matrix (5 pts)\n",
    "A confusion matrix is a fundamental tool for evaluating the performance of classification models. Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class.\n",
    "\n",
    "You are asked to evaluate your trained model by computing and printing the confusion matrix. You can either compute it by yourself or use sklearn.metrics.confusion_matrix()."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "giA5Kg53DBwX"
   },
   "source": [
    "# TODO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdkvCp5bDVe8"
   },
   "source": [
    "## Observations (15 pts)\n",
    "Write down your observations regarding the results you obtained throughout this assignment. Here are some suggestions:\n",
    "* **Accuracy and Loss Curves**: Plot and compare the training and validation accuracy and loss curves for each model. This helps visualize how well each model is learning over time and whether they are overfitting or underfitting.\n",
    "* **Top Misclassified Images**: Examine the classes that are most frequently misclassified by each model. This can provide insights into the types of images that are challenging for each model and may suggest areas for improvement.\n",
    "* **Feature Visualization**: Visualize the feature maps or activations of intermediate layers in each CNN. This can help you understand what features or patterns each model is learning and whether they differ in terms of learned representations.\n",
    "* **Robustness Testing**: Assess the robustness of each model by introducing noise, transformations, or adversarial examples to the test data. This can help identify which models are more resilient to perturbations.\n",
    "* **Runtime and Resource Usage**: Compare the training time and resource usage (e.g., GPU memory) of each model.\n",
    "* **Hyperparameter Tuning**: Analyze the impact of hyperparameters (learning rates, batch sizes, etc.) on training speed and convergence.\n",
    "* **Model Size and Efficiency**: Analyze the trade-off between model size and accuracy for each model.\n",
    "* **Ablation Studies**: Conduct ablation studies by removing or modifying specific components (e.g., dropout, batch normalization, etc.) of each model to understand their contributions to performance.\n",
    "\n",
    "You don't need to follow them. Feel free to write down any observation you have, or to use tools like Tensorboard to support your observations. You are also welcome to give comments on the design of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIDgYVOrGcZI"
   },
   "source": [
    "## **TODO: write down your observations**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
